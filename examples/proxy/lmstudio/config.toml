# Example config.toml for connecting tinyMem to LM Studio

# This file should be placed in your project's .tinyMem/ directory
# (e.g., /path/to/your-project/.tinyMem/config.toml)

[proxy]
# The port on which the tinyMem proxy server will run.
port = 8080

# The base URL of your local LLM provider.
# This is the default API endpoint for LM Studio.
base_url = "http://localhost:1234/v1"
# LM Studio's documentation (2025) confirms this endpoint and notes you can override it with LMSTUDIO_API_BASE when you start the server.

# To use this, you must first start the local server in LM Studio.
# 1. Go to the "Local Server" tab (the "↔️" icon) in LM Studio.
# 2. Click the "Start Server" button.

# All other tinyMem settings can also be configured in this file.
# For a full list of options, see the main README.md file.
