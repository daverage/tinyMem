# Example config.toml for connecting tinyMem to Ollama

# This file should be placed in your project's .tinyMem/ directory
# (e.g., /path/to/your-project/.tinyMem/config.toml)

[proxy]
# The port on which the tinyMem proxy server will run.
port = 8080

# The base URL of your local LLM provider.
# This is the default API endpoint for Ollama.
# Ollama's HTTP API defaults to `http://localhost:11434` (the OpenAI compatibility layer uses `/v1`), so this base_url matches their published endpoint in late 2025.
base_url = "http://localhost:11434/v1"

# All other tinyMem settings can also be configured in this file.
# For a full list of options, see the main README.md file.
