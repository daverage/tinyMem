# Example config.toml for LM Studio + Qwen2.5-Coder
# Optimized for qwen2.5-coder-7b-instruct

[proxy]
# The port on which the tinyMem proxy server will run.
port = 8080

# The base URL of LM Studio (default is 1234)
base_url = "http://localhost:1234/v1"

[llm]
# Explicitly set the base URL for internal tinyMem calls (like CoVe or extraction)
base_url = "http://localhost:1234/v1"
# Specify the model name loaded in LM Studio
model = "qwen2.5-coder-7b-instruct"

[recall]
# Qwen-Coder is good with context, but keeping recall items reasonable helps speed
max_items = 10
max_tokens = 4096

[cove]
# Enable Chain-of-Verification for higher accuracy with Qwen
enabled = true
# Qwen2.5-Coder-7B is capable enough for CoVe
confidence_threshold = 0.7