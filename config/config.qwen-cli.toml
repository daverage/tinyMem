# tinyMem v5.3 (Gold) Configuration
# Per Specification: minimal and boring, no tuning knobs, no feature flags
# All fields are REQUIRED unless explicitly noted

[database]
# Path to SQLite database file
# The database will be created if it doesn't exist
database_path = "./runtime/tinyMem.db"

[logging]
# Path to log file
log_path = "./runtime/tinyMem.log"

# Enable debug logging (true/false)
debug = true

[llm]
llm_provider = "ollama"  # Changed to ollama to match the endpoint
llm_endpoint = "http://127.0.0.1:11434/v1"  # Base API endpoint URL
llm_api_key = "ollama"  # API key (can be any string for ollama)
llm_model = "qwen:7b"  # Model identifier for ollama

[server]
port = 4321
host = "127.0.0.1"

[context]
recentContextPairs = 5
maxContextTokens = 8000

[proxy]
# Address and port for the local proxy server
# Format: "host:port"
# The proxy will listen on this address for OpenAI-compatible requests
# Default: Port 4321
# Endpoint: http://{listen_address}/v1/chat/completions
listen_address = "127.0.0.1:4322"

[hydration]
# Hydration Budget Limits
# Set to 0 for unlimited
max_tokens = 32000      # Maximum tokens to hydrate (prevents context overflow)
max_entities = 50       # Maximum entities to hydrate

# Structural Anchors (Deterministic Retrieval)
# These should always be enabled for trustworthy retrieval
enable_file_mention_anchors = true      # Include entities from explicitly mentioned files
enable_symbol_mention_anchors = true     # Include entities matching mentioned symbols
enable_previous_hydration_anchors = true # Include previously hydrated entities

# Semantic Ranking (Optional)
# Uses embeddings to find semantically similar code
enable_semantic_ranking = true          # Set to true to enable semantic search
semantic_threshold = 0.7                 # Cosine similarity cutoff (0.0 to 1.0)
semantic_budget_tokens = 8000            # Max tokens for semantic expansion
semantic_budget_entities = 10            # Max entities from semantic ranking

# Embedding Provider
# Options: "simple" (hash-based, for testing), "openai" (requires API key), "none"
embedding_provider = "simple"
embedding_model = "simple-384"           # Model identifier
embedding_cache_ttl = 86400              # Cache TTL in seconds (24 hours)
